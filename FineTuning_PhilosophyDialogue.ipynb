{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a724f1da-0bf0-492e-919f-d8f5948259f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DS 5110 Assignment 4: Part 2: Fine-tuning Script for the GPT model series\n",
    "\n",
    "This script loads a pretrained GPT2 model checkpoint and fine-tunes it on a specific dataset\n",
    "using the HuggingFace Transformers library's Trainer API.\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "DO NOT MODIFY THIS CELL.\n",
    "This section contains essential imports for fine-tuning GPT models.\n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    GPT2Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    set_seed\n",
    ")\n",
    "# Add safetensors import\n",
    "try:\n",
    "    from safetensors.torch import save_file\n",
    "    SAFETENSORS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SAFETENSORS_AVAILABLE = False\n",
    "    print(\"SafeTensors not available. Install with: pip install safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8e5da1b-d6a2-4f4c-ad3f-cce208f24a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DO NOT MODIFY THIS CELL.\n",
    "This cell loads the model and tokenizer.\n",
    "'''\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer either from checkpoint or from HF hub.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the pretrained model checkpoint\n",
    "        model_name: Base model architecture to use if checkpoint not found\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer) - The loaded model and tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Your code here\n",
    "    # Hint. Load the tokenizer from model_name\n",
    "    # Hint. Set padding token to eos_token if not set\n",
    "    # Hint. Try to load model from checkpoint_path, handling PT files if needed\n",
    "    # Hint. Fall back to base model if checkpoint loading fails\n",
    "    # Always load the tokenizer from model_name (more reliable)\n",
    "    # Load tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Initialize model architecture\n",
    "    #config = GPT2Config.from_pretrained(model_name)\n",
    "    \n",
    "    # Initialize model with this config\n",
    "    print(f\"Load a pretrained GPT2 model\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b9bb82-60b6-4aab-b399-1c0a0cc3cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell prepares the dataset.\n",
    "'''\n",
    "def prepare_dataset(dataset_name, tokenizer, dataset_config=None):\n",
    "    \"\"\"    Load and prepare dataset for fine-tuning.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the Hugging Face dataset to use\n",
    "        dataset_config: Specific configuration of the dataset\n",
    "        tokenizer: The tokenizer to use for preprocessing\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_dataset, eval_dataset) - Tokenized datasets for training and evaluation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataset = load_dataset(dataset_name, dataset_config)\n",
    "        print(f\"Dataset loaded with config: {dataset}\")\n",
    "    except:\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        print(f\"Dataset loaded without config: {dataset}\")\n",
    "    \n",
    "    # Prepare train and validation splits\n",
    "    if 'validation' in dataset:\n",
    "        train_dataset = dataset['train']\n",
    "        eval_dataset = dataset['validation']\n",
    "    else:\n",
    "        # Create a validation split if none exists\n",
    "        train_eval = dataset['train'].train_test_split(test_size=0.1)\n",
    "        train_dataset = train_eval['train']\n",
    "        eval_dataset = train_eval['test']\n",
    "\n",
    "    \"\"\"\n",
    "    TODO: Task 2.1\n",
    "    \"\"\"\n",
    "    # Hint: Explicitly set the dataset range to N\n",
    "    # Hint: where 50 <= N <= 200\n",
    "\n",
    "    # TODO: Your code here... (uncomment the following two statements and configure N)\n",
    "    #N = 50\n",
    "    # I commented this again because I want to utilize the whole dataset\n",
    "    #train_dataset = train_dataset.select(range(N))\n",
    "    #eval_dataset = eval_dataset.select(range(N))\n",
    "    \"\"\"\n",
    "    End of Task 2.1\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    print(f\"Training examples: {len(train_dataset)}\")\n",
    "    print(f\"Evaluation examples: {len(eval_dataset)}\")\n",
    "    column_names = train_dataset.column_names\n",
    "    print(f\"Dataset columns: {column_names}\")\n",
    "\n",
    "    # Define tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        # For alpaca style datasets with input/output columns\n",
    "        if 'instruction' in column_names and 'output' in column_names: # This is changed to match the philosophy dialogue\n",
    "            # Combine input and output for language modeling\n",
    "            texts = [\n",
    "                f\"Input: {inp}\\nOutput: {out}\"\n",
    "                for inp, out in zip(examples['instruction'], examples['output']) # Maybe this should not have been changed\n",
    "            ]\n",
    "            \n",
    "        return tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "    # Tokenize the datasets\n",
    "    tokenized_train = train_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    tokenized_eval = eval_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    return tokenized_train, tokenized_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f288f0cb-071c-4e67-9690-2cb080b40bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL.\n",
    "This cell: \n",
    "1. Sets up the TrainingArguments with appropriate parameters\n",
    "2. Initializes the Trainer with the model, datasets, and training arguments\n",
    "3. Configures a data collator for language modeling\n",
    "\"\"\"\n",
    "def setup_trainer(model, tokenizer, train_dataset, eval_dataset, output_dir, learning_rate, batch_size, max_steps):\n",
    "    \"\"\"\n",
    "    Set up the Trainer for fine-tuning.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to fine-tune\n",
    "        tokenizer: The tokenizer used for preprocessing\n",
    "        train_dataset: The training dataset\n",
    "        eval_dataset: The evaluation dataset\n",
    "        output_dir: Directory to save the fine-tuned model\n",
    "        learning_rate: Learning rate for fine-tuning\n",
    "        batch_size: Batch size for training\n",
    "        max_steps: Maximum number of training steps\n",
    "        \n",
    "    Returns:\n",
    "        Trainer: Configured Trainer instance\n",
    "    \"\"\"\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False  # We're doing causal language modeling, not masked LM\n",
    "    )\n",
    "    \n",
    "    # Set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        run_name=\"my-finetune-run\",\n",
    "        report_to=\"none\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        max_steps=max_steps,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_steps=100,\n",
    "        warmup_steps=100,\n",
    "        logging_steps=10,\n",
    "        gradient_accumulation_steps=4,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        load_best_model_at_end=True\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b412b857-069c-4d33-a21d-fe7182225d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL.\n",
    "This cell: \n",
    "1. Starts the fine-tuning process using the trainer\n",
    "2. Saves the fine-tuned model and tokenizer\n",
    "3. Handles any interruptions gracefully\n",
    "\"\"\"\n",
    "def run_fine_tuning(trainer, output_dir, tokenizer):\n",
    "    \"\"\"\n",
    "    Run the fine-tuning process and save the model.\n",
    "    \n",
    "    Args:\n",
    "        trainer: The configured Trainer instance\n",
    "        output_dir: Directory to save the fine-tuned model\n",
    "        tokenizer: The tokenizer to save alongside the model\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if fine-tuning completed successfully, False otherwise\n",
    "    \"\"\"\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model and tokenizer\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a04996c-4063-4f57-a8fc-66c68a628049",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell: \n",
    "1. Load the fine-tuned model and tokenizer\n",
    "2. Create a text generation pipeline\n",
    "3. Generate sample text to verify the model's performance\n",
    "You could tune the configurations for model.generate() to see how differently the model performs.\n",
    "\"\"\"\n",
    "def test_fine_tuned_model(model_path, prompt=\"Once upon a time, \"):\n",
    "    \"\"\"\n",
    "    Generate sample text with the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the fine-tuned model\n",
    "        prompt: Text prompt to start generation\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated text\n",
    "    \"\"\"\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Use GPU if available; but most likely it'll use CPU as \n",
    "    # our EC2 instance is CPU-only \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate output\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,  # Set the max output token length\n",
    "        temperature=0.7, # Set how random the output is\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Decode and return generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be62378c-b708-4b78-90a3-e1511801a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell defines the configurations of the fine-tuning process.\n",
    "'''\n",
    "model_name = 'gpt2'    # By default, load pretrained GPT2 from Hugging Face\n",
    "#model_name = 'EleutherAI/pythia-70m'\n",
    "#dataset_name = 'wikitext'\n",
    "dataset_name = 'Hypersniper/philosophy_dialogue' # Use the alpaca_1k Q&A dataset first\n",
    "dataset_config = None\n",
    "\n",
    "\"\"\"\n",
    "TODO: Task 2.2\n",
    "\"\"\"\n",
    "# Hint: You need to change output_dir to a different directory so that \n",
    "# Hint: when you start a new fine-tuning, it won't overwrite your existing\n",
    "# Hint: fine-tuned model\n",
    "\n",
    "output_dir = 'fine-tuned-model-3' # The output directory where the fine-tuned model is saved\n",
    "\"\"\" \n",
    "End of Task 2.2\n",
    "\"\"\"\n",
    "max_steps = 100        # Max number of training steps\n",
    "batch_size = 4         # Batch size is set to 4 to bound your DRAM usage to ~60%; increasing this would lead to OOM (out of memory)\n",
    "learning_rate = 1e-5   \n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5f653d8-5d9e-4014-80b2-b7acc7174aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load a pretrained GPT2 model\n",
      "Dataset loaded with config: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'output'],\n",
      "        num_rows: 395\n",
      "    })\n",
      "})\n",
      "Training examples: 355\n",
      "Evaluation examples: 40\n",
      "Dataset columns: ['instruction', 'output']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fc450dd8ce4ca0b32b62011491320b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d10ab32a3d4fd9b1eabd8a314700d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up trainer...\n",
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 56:16, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.476000</td>\n",
       "      <td>2.334355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text is What is the meaning of life?\n",
      "Life is not a simple question. It is an exploration of our lives, our emotions, our relationships, our relationships with others, and our understanding of ourselves. It is a journey that we must all share, not just for ourselves, but for all of us.\n",
      "\n",
      "But it is not the only life. It is also a journey that we must all strive for, for we must strive for our own happiness, for we must strive for our own fulfillment\n",
      "Fine-tuning complete! Model saved to fine-tuned-model-3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run the entire fine-tuning process.\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "TODO: Task 1\n",
    "\"\"\"\n",
    "# Hint: Set seed for reproducibility by calling set_seed()\n",
    "# Hint: Load model and tokenizer by calling load_model_and_tokenizer()\n",
    "# Hint: Prepare dataset\n",
    "# Hint: Set up trainer by calling setup_trainer()\n",
    "# Hint: Start the fine-tuning process by calling run_fine_tuning()\n",
    "\n",
    "# Hint: You should add reasonable print statements to help track the program running\n",
    "\n",
    "# TODO: Your code here...\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "\n",
    "# Prepped dataset\n",
    "\n",
    "tokenized_train, tokenized_eval = prepare_dataset(dataset_name, tokenizer)\n",
    "print(\"Setting up trainer...\")\n",
    "trainer = setup_trainer(model, tokenizer, tokenized_train, tokenized_eval, output_dir, learning_rate, batch_size, max_steps)\n",
    "print(\"Starting fine-tuning...\")\n",
    "run_fine_tuning(trainer, output_dir, tokenizer)\n",
    "\"\"\"\n",
    "End of Task 1\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Testing fine-tuned model...\")\n",
    "generated_text = test_fine_tuned_model(output_dir, prompt = 'What is the meaning of life?') # Using a paraphrased philosophical prompt\n",
    "print(f\"Generated text is {generated_text}\")\n",
    "print(f\"Fine-tuning complete! Model saved to {output_dir}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed6fac-76d4-4177-8739-99c54f4adc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc8031e-e8a6-46fc-9d42-393665385df0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
